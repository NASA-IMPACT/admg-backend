import json
import requests

from collections import namedtuple
from datetime import datetime
from io import BytesIO
from lxml import etree
from urllib.parse import urlencode


class QueryCounter:
    def __init__(self, page_num=1, page_size=100):
        self.page_num = page_num
        self.page_size = page_size
        self.finished = False

    def calculate_num_returned(self, num_hits, naive):
        """Calculates number of hits returned in the current CMR page

        Args:
            num_hits (int): num hits from api call metadata
            page_size (int): page size from user query
            page_num (int): page number from user query
            naive (bool): selects for naive calculation

        Returns:
            int: number of results on current page
        """

        if naive:
            num_returned = self.page_num * self.page_size
        else:
            num_returned = self.page_size * self.page_num
            if num_returned < num_hits:
                num_returned = num_hits % num_returned

        return num_returned

    def iterate(self, num_hits):
        """Updates self.page_num and self.finished after a new page with new hits
        is returned.

        Args:
            num_hits (int): number of hits returned from the most recent query
        """
        num_returned_naive = self.calculate_num_returned(num_hits, naive=True)
        if num_returned_naive < num_hits:
            self.page_num += 1
        else:
            self.finished = True


def get_xml(url):
    response = requests.get(url)
    rdf_file = BytesIO(response.content)
    tree = etree.parse(rdf_file)  # could fail for a lot of reasons
    return tree


def get_json(url):
    response = requests.get(url).text
    # TODO: address this in the testing
    try:
        data = json.loads(response)
    except:
        data = None
    return data


# def ingest_campaign(short_name):
def campaign_query(short_name):
    # TODO: i think I can update this to query several CMR parameters
    """Queries CMR for a specific campaign/project short_name and aggergates
    all the granule metadata.

    Args:
        short_name (str): capaign/project short_name

    Returns:
        list: list of xml_trees
    """

    # set initial variables
    counter = QueryCounter()
    base_url = 'https://cmr.earthdata.nasa.gov/search/collections?'
    campaign_trees = []

    while not counter.finished:
        # make inital query and append results
        parameters = urlencode({'project': short_name,
                                'page_size': counter.page_size,
                                'page_num': counter.page_num},
                                doseq=True)
        url = base_url + parameters

        campaign_tree = get_xml(url)
        campaign_trees.append(campaign_tree)

        # iterate counter
        num_hits = int(campaign_tree.find('hits').text)
        counter.iterate(num_hits)

    return campaign_trees


def campaign_xml_to_json(campaign_trees):
    """Accepts campaign metadata in the form of a list of campaign_tree xml files
    and parses out the relevant information, aggregates the results, and converts
    to a python dictionary.

    Args:
        campaign_trees ([list]): List of campaign xlm files

    Returns:
        dict: Parsed dictionary of the original campaign xml files
    """

    concept_ids = []
    for campaign_tree in campaign_trees:
        for references in campaign_tree.findall('references'):
            for reference in references:
                concept_ids.append(reference.find('id').text)

    # set initial variables
    counter = QueryCounter()
    base_url = 'https://cmr.earthdata.nasa.gov/search/collections.umm_json?'
    metadata = []

    while not counter.finished:
        parameters = urlencode({'echo_collection_id[]': concept_ids,
                                'page_size': counter.page_size,
                                'page_num': counter.page_num},
                                doseq=True)
        url = base_url + parameters

        data = get_json(url)
        metadata += [{'concept_id': entry['meta']['concept-id'], 'metadata': entry['umm']} for entry in data['items']]

        # iterate counter
        num_hits = int(data['hits'])
        counter.iterate(num_hits)

    return metadata


def general_extractor(campaign_metadata, field):
    """Extracts and aggregates a specific top level item from the campaign_metadata.

    Args:
        campaign_metadata (dict): this is a custom dict generated by campagin_xml_json
        field (str): Key value for the desired field name

    Returns:
        list: Aggregated list of the values in the requested field.
    """

    data = []
    for reference in campaign_metadata:
        value = reference['metadata'].get(field, '')
        if value:
            data.append(value)
    return data


def extract_daacs(campaign_metadata):
    """Extracts and aggregates all the daacs from a single campaign.

    Args:
        campaign_metadata ([type]): [description]

    Returns:
        list: list of daac names
    """

    # TODO: inventory team should specify which role they are most concerned with
    role_filter=['ARCHIVER', 'DISTRIBUTOR', 'PROCESSOR', 'ORIGINATOR']

    mega_daac_list=general_extractor(campaign_metadata, 'DataCenters')
    daacs = [
        daac['ShortName']
            for daac_list in mega_daac_list
                for daac in daac_list
                    if daac['Roles'][0] in role_filter
            ]

    return daacs


def extract_region_description(campaign_metadata):
    """Extracts the GCMD LocationKeywords from the metadata. These will be
    used as a proxy for region_description at the campaign level.

    Args:
        campaign_metadata ([type]): [description]

    Returns:
        list: [{'Category': value, 'Type': value, 'Subregion1': value, 'Subregion2': value}]
    """

    nested_regions = general_extractor(campaign_metadata, 'LocationKeywords')

    # json.dumps allows us to take the set of the dictionaries
    # the list comprehension is unpacking the nested entries
    regions_json = set([json.dumps(region)
                    for region_list in nested_regions
                        for region in region_list])
    regions_dict = [json.loads(region) for region in regions_json]

    return regions_dict


def extract_collection_periods(campaign_metadata):
    """Extracts platforms as a proxy for collection_periods and aggregates the metadata

    Args:
        campaign_metadata (dict): this is a custom dict generated by campagin_xml_json

    Returns:
        dict: A dict containing each unique platform and its aggregated metadata, including
            DOIs, short and long names, platform identifiers, and instruments.
    """

    platforms = {}
    for data_product in campaign_metadata:
        print(data_product)

        # TODO: refactor the error handling in this loop 
        for platform_info in data_product['metadata'].get('Platforms', [{}]):
            # generate reference name
            platform_short_name = platform_info.get('ShortName', '')
            platform_long_name = platform_info.get('LongName', '')

            platform_chars = data_product['metadata']['Platforms'][0].get('Characteristics', [])
            platform_identifiers = [char.get('Value', '') for char in platform_chars if char.get('Name') == 'AircraftID']

            platform_reference = '_&_'.join([platform_short_name, platform_long_name] + platform_identifiers)

            # get instruments
            instruments = {}
            if platform_info.get('Instruments'):
                for instrument_info in platform_info.get('Instruments', []):
                    instrument_short_name = instrument_info.get('ShortName', '')
                    instrument_long_name = instrument_info.get('LongName', '')
                    instrument_reference = '_&_'.join([instrument_short_name, instrument_long_name])

                    instruments[instrument_reference] = {
                        'short_name': instrument_short_name,
                        'long_name': instrument_long_name,
                    }

            if platforms.get(platform_reference):
                platforms[platform_reference]['instruments'] = {
                    **platforms[platform_reference]['instruments'],
                    **instruments}
                platforms[platform_reference]['dois'].append(data_product.get('metadata',{}).get('DOI'))

            else:
                platforms[platform_reference] = {
                    'platform_names': {
                        'short_name': platform_short_name,
                        'long_name': platform_long_name},
                    'platform_identifiers': platform_identifiers,
                    'instrument_information_source': 'cmr_api',
                    'auto_generated': True,
                    'instruments': instruments,
                    'dois': [data_product.get('metadata',{}).get('DOI')]
                }

    return platforms


def date_overlap(cmr_start, cmr_end, dep_start, dep_end):
    """Takes two date ranges and returns the number of days of overlap.

    Args:
        cmr_start (datetime): Start date for CMR data
        cmr_end (datetime): End date for CMR data
        dep_start (datetime): Start date for deployment
        dep_end (datetime): End date for deployment

    Returns:
        int: Number of days of overlap of the two date ranges, or 0 if None.
    """

    Range = namedtuple('Range', ['start', 'end'])

    cmr_range = Range(start=cmr_start, end=cmr_end)
    dep_range = Range(start=dep_start, end=dep_end)

    latest_start = max(cmr_range.start, dep_range.start)
    earliest_end = min(cmr_range.end, dep_range.end)

    delta = (earliest_end - latest_start).days + 1
    overlap = max(0, delta)

    return overlap


def date_filter(campaign_metadata, dep_start, dep_end):
    """This function is intended to filter the returned CMR metadata based on a
        date range. It is expected to be used to generate deployment metadata, since
        CMR does not distinguish deployments and the user must input this data.

    Args:
        campaign_metadata (dict): this is a custom dict generated by campagin_xml_json
        dep_start (datetime): Start date for deployment
        dep_end (datetime): End date for deployment

    Returns:
        dict: This is a filtered version of the original metadata given to the function.
    """

    date_format = '%Y-%m-%dT%H:%M:%S.%fZ'

    filtered_metadata = []
    for reference in campaign_metadata:
        temporal_extents = reference['metadata'].get('TemporalExtents', [{}])[0]

        cmr_start = temporal_extents.get('RangeDateTimes', [{}])[0].get('BeginningDateTime', 'error')
        cmr_end = temporal_extents.get('RangeDateTimes', [{}])[0].get('EndingDateTime', 'error')
      
        if cmr_start == 'error' or cmr_end == 'error':
            print('        ', reference['concept_id'], "failed (full date couldn't be extracted)")
            continue

        try:
            cmr_start = datetime.strptime(cmr_start, date_format)
            cmr_end = datetime.strptime(cmr_end, date_format)
            print('        ', reference['concept_id'], 'success')            
        except ValueError:
            # might be able to update the code to capture some of these incorrectly formatted dates
            # example: time data '2002-03-21T00:00:00Z' does not match format '%Y-%m-%dT%H:%M:%S.%fZ'
            print('        ', reference['concept_id'], "failed (date was in wrong format)")
            continue

        days_overlapping = date_overlap(cmr_start, cmr_end, dep_start, dep_end)
        if days_overlapping > 0:
            filtered_metadata.append(reference)

    return filtered_metadata


def project_filter(campaign_metadata, short_name):
    """Filters the campaign_metadata on a given project short_name.

    Args:
        campaign_metadata (dict): this is a custom dict generated by campagin_xml_json
        short_name (str): Project short name

    Returns:
        dict: This is a filtered version of the original metadata given to the function.
    """
    # TODO: does it make sense to use any()?
    filtered_metadata = []
    for reference in campaign_metadata:
        projects = reference['metadata'].get('Projects', [])
        for project in projects:
            project_short_name = project.get('ShortName', '')
            if short_name.lower() == project_short_name.lower():
                filtered_metadata.append(reference)
                break
    return filtered_metadata


def combine_spatial_extents(spatial_extents):
    # TODO: this should combine multiple spatial extents into a total coverage 
    pass
